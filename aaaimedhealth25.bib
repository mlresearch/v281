@Proceedings{AIMedHealth-2025,
    booktitle = {Proceedings of The First AAAI Bridge Program on AI for Medicine and Healthcare},
    name = {AAAI Bridge Program on AI for Medicine and Healthcare},
    shortname = {AIMedHealth},
    year = {2025},
    editor = {Wu, Junde and Zhu, Jiayuan, and Xu, Min and Jin, Yueming},
    volume = {281},
    start = {2025-2-25},
    end = {2025-2-25},
    published = {2025-04-22},
    address = {Pennsylvania Convention Center, Philadelphia, Pennsylvania, USA},
    conference_url = {https://sites.google.com/view/aimedhealth-aaai/home}, 
}

@InProceedings{zhu25,
    title = {AAAI Bridge Program 2025: AI for Medicine and Healthcare},
    author = {Zhu, Jiayuan and Xu, Min and Jin, Yueming and Novak, Alex and Ather, Sarim and Papiez, Bartlomiej W. and Li, Xiang and Wu, Junde},
    pages = {1-4},
    abstract = {The AI for Medicine and Healthcare bridge program at AAAI-25 was designed to address the critical gap between the rapid advancement of AI technologies and their effective integration into clinical practice. While AI has demonstrated significant success in medical imaging, diagnostics, and workflow automation, concerns regarding explainability, reliability, and clinical usability continue to hinder widespread adoption. The bridge program provided an interdisciplinary platform for AI researchers, clinicians, and industry experts to collaborate, exchange insights, and discuss strategies for developing AI-driven healthcare solutions that align with real-world medical needs. The event featured research paper presentations, paper poster sessions, and expert panel discussions. This paper provides an in-depth review of the key themes, challenges, and future directions that emerged from AI for Medicine and Healthcare bridge program at AAAI-25.}
}

@InProceedings{gupta25,
    title = {FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein Estimator},
    author = {Gupta, Sunny and Jangid, Nikita and Sethi, Amit},
    pages = {5-13},
    abstract = {Federated Learning (FL) facilitates data privacy by enabling collaborative in-situ training across decentralized clients. Despite its inherent advantages, FL faces significant challenges of performance and convergence when dealing with data that is not independently and identically distributed (non-i.i.d.). While previous research has primarily addressed the issue of skewed label distribution across clients, this study focuses on the less explored challenge of multi-domain FL, where client data originates from distinct domains with varying feature distributions. We introduce a novel method designed to address these challenges – FedStein: Enhancing Multi-Domain Federated Learning Through the James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS) estimates of batch normalization (BN) statistics across clients, while maintaining local BN parameters. The non-BN layer parameters are exchanged via standard FL techniques. Extensive experiments conducted across three datasets and multiple models demonstrate that FedStein surpasses existing methods such as FedAvg and FedBN, with accuracy improvements exceeding 14\% in certain domains leading to enhanced domain generalization. The code is available at https://github.com/sunnyinAI/FedStein}
}

@InProceedings{shang25,
    title = {An Effective Transformer-based Solution for RSNA Intracranial Hemorrhage Detection Competition},
    author = {Shang, Fangxin and Wang, Siqi and Wang, Xiaorong and Yang, Yehui},
    pages = {14-19},
    abstract = {We propose an effective method for the detection of intracranial hemorrhages (IHD) that exceeds the performance of the winning solution in the RSNA-IHD competition (2019)(Anouk Stein et al. 2019). Meanwhile, our model only takes quarter parameters and ten percent FLOPs compared to the winner’s solution. The IHD task must predict the hemorrhage category of each slice for the input brain CT. We review the top five solutions for the IHD competition held by the Radiological Society of North America(RSNA) in 2019. Almost all the top solutions rely on 2D convolutional networks and sequential models (Bidirectional GRU or LSTM) to extract intraslice and interslice features, respectively. All the top solutions improve performance by using the ensemble of models, and the number of models varies from 7 to 31. In the past years, since much progress has been made in the computer vision regime especially Transformer-based models, we introduce the Transformer-based techniques to extract the features in both intra-slice and inter-slice views for IHD tasks. Additionally, a semi-supervised method is embedded into our workflow to further improve the performance. The code is already available online. Code — https: //github.com/PaddlePaddle/Research/tree/master/CV. Datasets — https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection}
}

@InProceedings{wang25,
    title = {Proactive Pseudo-Intervention: Pre-informed Contrastive Learning For Interpretable Vision Models},
    author = {Wang, Dong and Yang, Yuewei and Chen, Liqun and Gan, Zhe and Henao, Ricardo and Carin, Lawrence},
    pages = {20-34},
    abstract = {Deep neural networks excel at comprehending complex visual signals, delivering on par or even superior performance to that of human experts. However, ad-hoc visual explanations of model decisions often reveal an alarming level of reliance on exploiting non-causal visual cues that strongly correlate with the target label in training data. As such, deep neural nets suffer compromised generalization to novel inputs collected from different sources, and the reverse engineering of their decision rules offers limited interpretability. To overcome these limitations, we present a novel contrastive learning strategy called Proactive Pseudo-Intervention (PPI) that leverages proactive interventions to guard against image features with no causal relevance. We also devise a novel pre-informed salience mapping module to identify key image pixels to intervene and show it greatly facilitates model interpretability. To demonstrate the utility of our proposals, we benchmark it on both standard natural images and challenging medical image datasets. PPI-enhanced models consistently deliver superior performance relative to competing solutions, especially on out-of-domain predictions and data integration from heterogeneous sources. Further, saliency maps of models that are trained in our PPI framework are more succinct and meaningful.}
}

@InProceedings{chen25,
    title = {Unsupervised Patch-GAN with Targeted Patch Ranking for Fine-Grained Novelty Detection in Medical Imaging},
    author = {Chen, Jingkun and Yang, Guang and Zhang, Xiao and Peng, Jingchao and Zhang, Tianlu and Zhang, Jianguo and Han, Jungong and Grau, Vicente},
    pages = {35-41},
    abstract = {Detecting novel anomalies in medical imaging is challenging due to the limited availability of labeled data for rare abnormalities, which often display high variability and subtlety. This challenge is further compounded when small abnormal regions are embedded within larger normal areas, as whole-image predictions frequently overlook these subtle deviations. To address these issues, we propose an unsupervised Patch-GAN framework designed to detect and localize anomalies by capturing both local detail and global structure. Our framework first reconstructs masked images to learn fine-grained, normal-specific features, allowing for enhanced sensitivity to minor deviations from normality. By dividing these reconstructed images into patches and assessing the authenticity of each patch, our approach identifies anomalies at a more granular level, overcoming the limitations of whole-image evaluation. Additionally, a patch-ranking mechanism prioritizes regions with higher abnormal scores, reinforcing the alignment between local patch discrepancies and the global image context. Experimental results on the ISIC 2016 skin lesion and BraTS 2019 brain tumor datasets validate our framework’s effectiveness, achieving AUCs of 95.79\% and 96.05\%, respectively, and outperforming three state-of-the-art baselines.}
}

@InProceedings{zhang25a,
    title = {One-stage Contour Regression Network for Muscle Segmentation},
    author = {Zhang, Hongyuan and Cai, Sijin and Wang, Xiaofeng and Zhou, Yongjin},
    pages = {42-48},
    abstract = {Muscle segmentation from ultrasound images is crucial for diagnosing and treating musculoskeletal diseases, as it provides geometric properties, such as area and thickness. However, most existing methods often rely on pixel-based segmentation networks to extract the muscle region. This is wasteful, inefficient, failing to meet the real-time requirements of dynamic analysis. Therefore, to address these issues, we introduce a novel One-stage Contour-based Regression network, termed as OCRSeg, which formulates the muscle segmentation problem as contour regression in the vertical direction. In specific, we achieve this using a regression framework that is: 1) simple, consisting of an encoder for feature extraction and an MLP for regressing contour coordinates; 2) flexible, enforcing soft total variation constraints during training to ensure locally smooth muscle edges; and 3) optimized for band-like musculature. Extensive experimental results on two datasets demonstrate that our method outperforms other state-of-the-art segmentation methods in terms of accuracy and efficiency, achieving over 95.0\% in mask IoU score and a FPS over 100 in running speed. Furthermore, another key advantage is that OCRSeg can achieve better clinical biometrics estimation compared with other techniques. We hope our proposed framework can serve as a fundamental and strong baseline for muscle segmentation task.}
}

@InProceedings{hemmerling25,
    title = {Improving AI Interpretability for Multilingual Parkinson’s Disease Classification through Voice Analysis},
    author = {Hemmerling, Daria and Zakrzewski, Michal and Wodzinski, Marek and Dudek, Milosz and Gaciarz, Filip and Wojcik-Pedziwiatr, Magdalena and Orozco-Arroyave, Juan Rafael and Noth, Elmar and Sztaho, David and Rumezhak, Taras},
    pages = {49-55},
    abstract = {Addressing the imperative need for interpretability in medical models based on machine learning and artificial intelligence, our study focuses on the crucial task of Parkinson’s disease detection. In this paper, we introduce a vision transformer incorporating multilingual vowel phonations, achieving a classification accuracy of 89\%. To enrich the input representation for vision transformer, we utilized images of melspectrograms and regular spectrograms. The success of our model goes beyond performance metrics, as we strategically integrate explainable artificial intelligence techniques. The synergy between robust classification results and explainability underscores the effectiveness of our approach in opening the black-box nature of neural networks. This, in turn, contributes to enhanced medical decision-making and reinforces the potential of artificial intelligence in advancing diagnostic methodologies for Parkinson’s disease.}
}

@InProceedings{najda25,
    title = {An Explainable AI-Integrated Diagnostic System for Voice Analysis in Heart Failure Patients},
    author = {Najda, Mikolaj and Dudek, Milosz and Unold, Olgierd and Jadczyk, Tomasz and Swierz, Krzysztof and Swiatek, Grzegorz and Hemmerling, Daria},
    pages = {56-62},
    abstract = {Integrating Explainable Artificial Intelligence to analyse voice characteristics is an essential topic for future research. We explore the utility of tree-based machine learning models, including Random Forest, XGBoost, and LightGBM, in distinguishing between two groups: 100 participants with heart failure and 100 healthy controls. The acoustic features extracted from sustained vowel recordings are used to differentiate between the two groups. The evaluation shows that the Random Forest model performs better, especially with the vowel /i/, achieving Accuracy, Precision, Recall, and F1 score over 0.80. We investigate the interpretability of these models usingSHapleyAdditiveexPlanationsvalues,whichrevealthe essential acoustic features that influence model predictions and provide insights into their clinical relevance. This research highlights the potential of interpretable vocal biomarkers in remote monitoring and diagnosing heart failure.}
}

@InProceedings{sabu25,
    title = {Investigating General-Purpose Large Language Models for Patient Information Extraction: A Case Study on Real-World Cardiac MRI Reports},
    author = {Sabu, Sebin and Rajendran, Pavithra and Sheldon, Ewart Jonny and Zenonos, Alexandros and Patel, Shiren and Taylor, Andrew and Pope, Rebecca and Sebire, Neil},
    pages = {63-69},
    abstract = {Electronic Patient Record (EPR) systems within healthcare systems contains a significant volume of free text written by clinicians in the form of unstructured data, meaning access to timely, potential pertinent data signals is precluded. For a clinician to analyse information for a cohort of patients for research, information extracted from unstructured data needs to be mapped with the routinely collected standard structured information and this can require lot of manual work and time. This paper studies the potential capabilities of general-purpose Large Language Models (LLMs) in the context of, (1) practical deployment using limited CPU computing resources, (2) usefulness in the context of extracting patient information within healthcare settings and (3) does not require fine-tuning or train models from scratch. In particular, we have investigated the utility of prompt-based zero-shot predictions by adapting these models in a question answering framework, which is deployed and run within a secure on-premise environment with CPU servers for extracting ten years of retrospective data containing 15,376 Cardiac MRI reports. Results are evaluated on a ground-truth dataset containing 400 randomly selected reports across the ten year period with the best performance having an averaged F1-score of 97.83\%. Source code will be made available upon acceptance.}
}

@InProceedings{kawatra25,
    title = {Minimal Data Maximum Impact: Lessons Learned from Real-World Unstructured Data in Paediatric Care},
    author = {Kawatra, Jaskaran Singh and Sabu, Sebin and Rajendran, Pavithra and Baumgartner, Caroline and Vijayaraghavan, Avish and Sheldon, Ewart Jonny and Booth, John and Sebire, Neil and Patel, Shiren and Zenonos, Alexandros and Pope, Rebecca},
    pages = {70-78},
    abstract = {Digital health records contains significant volume of pertinent, routine information locked within unstructured texts. Current processes requires costly human annotation from a limited number of expert annotators with sufficient domain knowledge and clinician’s time for verification of the outcomes. Our proposed two-stage automated approach enables (1) training and validation of fine-tuned few-shot domain-specific models, firstly to retrieve relevant documents and then performing entity recognition on the retrieved document chunks for identifying correct span of texts based on the use case at hand and, (2) a ”shadow deployment” pipeline testing an end-to-end solution in a pre-production environment. Our shadow deployment pipeline uses Large Language Models (LLMs) as an explainer-in-the-loop and Natural Language Inference (NLI) based verification approach to reduce the dependency on having a clinician to validate the outcomes of the solution. In this paper, we describe the experiments and results of deploying and testing our proposed approach within a real-world paediatric healthcare setting with a focus on histopathology reports of tumours, that can help answer clinical questions in a timely manner.}
}

@InProceedings{nguyen25a,
    title = {Evaluating topological fitness of human brain-inspired sub-circuits in Echo State Networks},
    author = {Nguyen, Bach V. and Chen, Tianlong and Yang, Shu and Hou, Bojian and Shen, Li and Duong-Tran, Duy},
    pages = {79-89},
    abstract = {In recent years, an emerging trend in neuromorphic computing has centered around the use of brain connectomics as a blueprint for artificial neural networks. Connectomics-based neuromorphic computing has primarily focused on embedding human brain large-scale structural connectomes (SCs), as estimated from diffusion Magnetic Resonance Imaging (dMRI) modality, to echo-state networks (ESNs). A critical step in ESN embedding requires pre-determined read-in and read-out layers constructed by the induced subgraphs of the embedded reservoir. As a priori set of functional sub-circuits are derived from functional MRI (fMRI) modality, it is unknown, till this point, whether the embedding of fMRI-induced sub-circuits/networks onto SCs is well justified from the neuro-physiological perspective and ESN performance across a variety of tasks. This paper proposes a pipeline to implement and evaluate ESNs with various embedded topologies and processing/memorization tasks. To this end, we showed that different performance optimums highly depend on the neuro-physiological characteristics of these pre-determined fMRI-induced sub-circuits. In general, fMRI-induced sub-circuit-embedded ESN outperforms simple bipartite and various null models with feed-forward properties commonly seen in MLP for different tasks and reservoir criticality conditions. We provided a thorough analysis of the topological properties of pre-determined fMRI-induced sub-circuits and highlighted their graph-theoretical properties that play significant roles in determining ESN performance. Finally, we demonstrate the model’s performance in predicting epidemiological time-series COVID-19 datasets, showing the bio-inspired model’s potential in application to public health decision-making.}
}

@InProceedings{zhang25b,
    title = {ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation},
    author = {Zhang, Xiaoman and Zhou, Hong-Yu and Yang, Xiaoli and Banerjee, Oishi and Acosta, Juli\'an N. and Miller, Josh and Huang, Ouwen and Rajpurkar, Pranav},
    pages = {90-99},
    abstract = {AI-driven models have demonstrated significant potential in automating radiology report generation for chest X-rays. However, there is no standardized benchmark for objectively evaluating their performance. To address this, we present ReXrank, a public leaderboard and challenge for assessing AI-powered radiology report generation. Our framework incorporates ReXGradient, the largest test dataset consisting of 10,000 studies, and three public datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation assessment. ReXrank employs 8 evaluation metrics and separately assesses models capable of generating only findings sections and those providing both findings and impressions sections. By providing this standardized evaluation framework, ReXrank enables meaningful comparisons of model performance and offers crucial insights into their robustness across diverse clinical settings. Beyond its current focus on chest X-rays, ReXrank’s framework sets the stage for comprehensive evaluation of automated reporting across the full spectrum of medical imaging.}
}

@InProceedings{zhang25c,
    title = {SegX: Improving Interpretability of Clinical Image Diagnosis with Segmentation-based Enhancement},
    author = {Zhang, Yuhao and Zhu, Mingcheng and Luo, Zhiyao},
    pages = {100-108},
    abstract = {Deep learning-based medical image analysis faces a significant barrier due to the lack of interpretability. Conventional explainable AI (XAI) techniques, such as Grad-CAM and SHAP, often highlight regions outside clinical interests. To address this issue, we propose Segmentation-based Explanation (SegX), a plug-and-play approach that enhances interpretability by aligning the model’s explanation map with clinically relevant areas leveraging the power of segmentation models. Furthermore, we introduce Segmentation-based Uncertainty Assessment (SegU), a method to quantify the uncertainty of the prediction model by measuring the ’distance’ between interpretation maps and clinically significant regions. Our experiments on dermoscopic and chest X-ray datasets show that SegX improves interpretability consistently across mortalities, and the certainty score provided by SegU reliably reflects the correctness of the model’s predictions. Our approach offers a model-agnostic enhancement to medical image diagnosis towards reliable and interpretable AI in clinical decision-making. The implementation is publicly available at https://github.com/JasonZuu/SegX.}
}

@InProceedings{luo25,
    title = {ReXplain: Translating Radiology into Patient-Friendly Video Reports},
    author = {Luo, Luyang and Vairavamurthy, Jenanan and Zhang, Xiaoman and Kumar, Abhinav and Ter-Oganesyan, Ramon R. and Schroff, Stuart T. and Shilo, Dan and Hossain, Rydhwana and Moritz, Mike and Rajpurkar, Pranav},
    pages = {109-120},
    abstract = {Radiology reports, designed for efficient communication between medical experts, often remain incomprehensible to patients. This inaccessibility could potentially lead to anxiety, decreased engagement in treatment decisions, and poorer health outcomes, undermining patient-centered care. We present ReXplain (Radiology eXplanation), an innovative AI-driven system that translates radiology findings into patient-friendly video reports. ReXplain uniquely integrates a large language model for medical text simplification and text-anatomy association, an image segmentation model for anatomical region identification, and an avatar generation tool for engaging interface visualization. ReXplain enables producing comprehensive explanations with plain language, highlighted imagery, and 3D organ renderings in the form of video reports. To evaluate the utility of ReXplain-generated explanations, we conducted two rounds of user feedback collection from six board-certified radiologists. The results of this proof-of-concept study indicates that ReXplain could accurately deliver radiological information and effectively simulate one-on-one consultation, shedding light on enhancing patient-centered radiology with potential clinical usage. This work demonstrates a new paradigm in AI-assisted medical communication, potentially improving patient engagement and satisfaction in radiology care, and opens new avenues for research in multimodal medical communication.}
}

@InProceedings{lee25,
    title = {Using Foundation Models to Prescribe Patients Proper Antibiotics},
    author = {Lee, Simon A. and Halperin, Helio and Halperin, Yanai and Brokowski, Trevor and Chiang, Jeffrey N.},
    pages = {121-132},
    abstract = {The rise of antibiotic-resistant bacteria presents a significant global health threat by reducing the effectiveness of essential treatments. This study evaluates the potential of clinical decision support systems powered by biomedical language foundation models to enhance antibiotic stewardship using electronic health records (EHRs). We test several state-of-the-art models, focusing on predicting whether each of eight different antibiotics will be effective for an individual patient. Additionally, we emphasize interpretability, aiming to understand how the models make decisions, where they excel, and where they fall short. Unlike previous research, which primarily benchmarks accuracy metrics, we provide insights into both the successes and limitations of these models, offering clinical and non-clinical experts a clearer understanding of their current state and reliability. These findings highlight the potential of AI systems to combat this global health threat, as well as the need for further improvements to address the limitations of existing models. We hope this work offers valuable guidance for improving AI-driven decision support systems and leveraging these advanced models for other clinical applications. Code — https://github.com/Simonlee711/antibiotics-fm- benchmark. Datasets —https://physionet.org/content/mimiciv/3.1/.}
}

@InProceedings{an25,
    title = {DK-BEHRT: Teaching Language Models International Classification of Disease (ICD) Codes using Known Disease Descriptions},
    author = {An, Ulzee and Lee, Simon A. and Jeong, Moonseong and Gorla, Aditya and Chiang, Jeffrey N. and Sankararaman, Sriram},
    pages = {133-143},
    abstract = {The widespread digitization of healthcare and patient data has created new opportunities to explore machine learning techniques for improving patient care. The sheer scale of this data has particularly motivated the use of deep learning methods like BERT, which can learn robust representations of medical concepts from patient data without the need for direct supervision. Simultaneously, recent research has shown that language models (LMs) trained on scientific literature can capture strong domain-specific knowledge, including concepts highly relevant to healthcare. In this paper, we leverage two complementary sources of information—patient medical records and descriptive clinical text—to learn complex clinical concepts, such as diagnostic codes, more effectively. Although significant strides have been made in using language models with each data type individually, few studies have explored whether the domain expertise acquired from scientific text can provide a beneficial inductive bias when applied to learning from patient records. To address this gap, we propose the Domain Knowledge BEHRT (DK-BEHRT), a model that integrates disease description embeddings from domain-specific language models, like BioGPT, into the attention mechanisms of a BERT-based architecture. By incorporating these “knowledge” embeddings, we aim to enhance the model’s ability to understand the clinical concept (e.g. ICD Codes) more effectively and predict clinical outcomes with higher accuracy. We validate this approach on the MIMIC-IV dataset and find that incorporating specialized embeddings consistently improves predictive accuracy for clinical outcomes compared to using generic embeddings or training the base model from scratch.}
}

@InProceedings{sikora25,
    title = {ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer},
    author = {Sikora, Natalia and Manschke, Robert L. and Tang, Alethea M. and Dunstan, Peter and Harris, Dean A. and Yang, Su},
    pages = {144-154},
    abstract = {Colorectal cancer (CRC) ranks as the second leading cause of cancer-related deaths and the third most prevalent malignant tumour worldwide. Early detection of CRC remains problematic due to its non-specific and often embarrassing symptoms, which patients frequently overlook or hesitate to report to clinicians. Crucially, the stage at which CRC is diagnosed significantly impacts survivability, with a survival rate of 80-95\% for Stage I and a stark decline to 10\% for Stage IV. Unfortunately, in the UK, only 14.4\% of cases are diagnosed at the earliest stage (Stage I). In this study, we propose ColonScopeX, a machine learning framework utilizing explainable AI (XAI) methodologies to enhance the early detection of CRC and pre-cancerous lesions. Our approach employs a multimodal model that integrates signals from blood sample measurements, processed using the Savitzky-Golay algorithm for fingerprint smoothing, alongside comprehensive patient metadata, including medication history, comorbidities, age, weight, and BMI. By leveraging XAI techniques, we aim to render the model’s decision-making process transparent and interpretable, thereby fostering greater trust and understanding in its predictions. The proposed framework could be utilised as a triage tool or a screening tool of the general population. This research highlights the potential of combining diverse patient data sources and explainable machine learning to tackle critical challenges in medical diagnostics.}
}

@InProceedings{noshin25,
    title = {Integrating Social Determinants of Health in a Multi-Modal Deep Clustering Survival Model for Injury-Risk in Alzheimer’s and Related Dementia Patients},
    author = {Noshin, Kazi and Boland, Mary Regina and Hou, Bojian and He, Weiqing and Lu, Victoria and Shen, Li and Zhang, Aidong},
    pages = {155-164},
    abstract = {As our population ages, the prevalence of Alzheimer’s Disease and Related Dementias (ADRD) and its associated burdens continue to rise. Social Determinants of Health (SDOH) significantly influence both ADRD development and progression. Using Electronic Health Records (EHR) from a quaternary care academic medical center in a diverse urban setting, we investigated SDOH’s impact on multi-modal deep clustering survival machines. Our findings revealed that SDOH improved model performance across feature selection methods (DeepCox roll-out vs. SHAP DeepExplainer) and EHR clinical modalities (medication vs. laboratory). Additionally, Laboratory features proved more informative than medications for predicting injury-fall risk. Our results highlight SDOH’s crucial role in ADRD progression, particularly regarding injury-fall risk. We found that feature importance varied by selection method when analyzing multi-modality EHR data, with education emerging as a key SDOH factor among our top 10 features, underscoring its significance in ADRD progression.}
}

@InProceedings{nguyen25b,
    title = {Semi-Supervised Histopathology Image Segmentation with Feature Diversified Collaborative Learning},
    author = {Nguyen, Thanh-Huy and Vu, Nguyen Lan Vi and Nguyen, Hoang-Thien and Dinh, Quang-Vinh and Li, Xingjian and Xu, Min},
    pages = {165-172},
    abstract = {Histopathology image segmentation plays a critical role in advancing disease diagnosis, prognosis, and treatment planning. However, it presents significant challenges due to the complexity of tissue structures, staining variability, and low contrast between tissue classes. Semi-supervised learning, employed to mitigate annotation scarcity, introduces additional difficulties, such as managing noisy pseudo-labels while ensuring robust performance with limited supervision. Traditional collaborative training methods, commonly used in medical image segmentation, often face issues like model coupling—where models become overly dependent on each other, propagating similar errors—or confirmation bias, where networks reinforce initial mistakes by relying on inaccurate pseudo-labels. Existing frameworks designed to tackle these challenges often suffer from complex pipelines and require extensive pre-training but fail to address the noise characteristics inherent in such datasets. To balance the efficiency of traditional co-training methods with dual networks while enhancing segmentation accuracy on noisy histopathological data, we propose Feature Diversified Collaborative Learning (FDCL). Our work aims to design an effective feature diversification loss that encourages the feature representations of sub-networks to be distinct, ensuring they capture different information to exchange with each other, thereby avoiding suboptimal solutions or, even worse, falling into the coupling problem. We benchmark our method on two well-known histopathology datasets and achieve state-of-the-art results on the GlaS dataset with only 10\% of the labeled data. Code is available at https://github.com/vnlvi2k3/FDCL.}
}

@InProceedings{hardy25,
    title = {ReXTrust: A Model for Fine-Grained Hallucination Detection in AI-Generated Radiology Reports},
    author = {Hardy, Romain and Kim, Sung Eun and Ro, Du Hyun and Rajpurkar, Pranav},
    pages = {173-182},
    abstract = {The increasing adoption of AI-generated radiology reports necessitates robust methods for detecting hallucinations—false or unfounded statements that could impact patient care. We present ReXTrust, a novel framework for fine-grained hallucination detection in AI-generated radiology reports. Our approach leverages sequences of hidden states from large vision-language models to produce finding-level hallucination risk scores. We evaluate ReXTrust on a subset of the MIMIC-CXR dataset and demonstrate superior performance compared to existing approaches, achieving an AUROC of 0.8751 across all findings and 0.8963 on clinically significant findings. Our results show that white-box approaches leveraging model hidden states can provide reliable hallucination detection for medical AI systems, potentially improving the safety and reliability of automated radiology reporting.}
}

@InProceedings{huang25,
    title = {RadRevise: A Benchmark Dataset for Instruction-Based Radiology Report Editing},
    author = {Huang, Yixuan and Acosta, Juli\'an Nicol\'as and Rajpurkar, Pranav},
    pages = {183-194},
    abstract = {Large Language Models (LLMs) can assist radiologists by making precise edits to radiology reports based on human instructions. However, evaluating the quality of such modifications has been challenging due to the lack of publicly available datasets. To address this gap, we present RadRevise, a novel dataset for assessing models’ ability to modify radiology reports according to specific instructions. RadRevise is derived from the radiology reports in the MIMIC-CXR dataset and includes 6,402 instructions and 2,922 modified reports. For each report, the dataset includes a set of one to five modification instructions, along with the corresponding modified output, covering various clinical topics and instruction types. Our benchmarking of current open-source models reveals performance gaps in accurately executing these instructions, highlighting areas for improvement in AI-assisted report modification.}
}

@InProceedings{zuo25a,
    title = {KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge Graph Enhancement for Medical Diagnosis},
    author = {Zuo, Kaiwen and Jiang, Yirui and Mo, Fan and Lio, Pietro},
    pages = {195-204},
    abstract = {Integrating Large Language Models (LLMs) in healthcare diagnosis demands systematic frameworks that can handle complex medical scenarios while maintaining specialized expertise. We present KG4Diagnosis, a novel hierarchical multi-agent framework that combines LLMs with automated knowledge graph construction, encompassing 362 common diseases across medical specialties. Our framework mirrors real-world medical systems through a two-tier architecture: a general practitioner (GP) agent for initial assessment and triage, coordinating with specialized agents for in-depth diagnosis in specific domains. The core innovation lies in our end-to-end knowledge graph generation methodology, incorporating: (1) semantic-driven entity and relation extraction optimized for medical terminology, (2) multi-dimensional decision relationship reconstruction from unstructured medical texts, and (3) human-guided reasoning for knowledge expansion. KG4Diagnosis serves as an extensible foundation for specialized medical diagnosis systems, with capabilities to incorporate new diseases and medical knowledge. The framework’s modular design enables seamless integration of domain-specific enhancements, making it valuable for developing targeted medical diagnosis systems. We provide architectural guidelines and protocols to facilitate adoption across medical contexts.}
}

@InProceedings{zuo25b,
    title = {MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models},
    author = {Zuo, Kaiwen and Jiang, Yirui},
    pages = {205-213},
    abstract = {Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations—generating medically implausible or inaccurate information—presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations, and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards.We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.}
}

